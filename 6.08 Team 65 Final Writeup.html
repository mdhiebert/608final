<meta charset="utf-8" emacsmode="-*- markdown -*-"><link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/apidoc.css?">

                          **6.08 Team 65 Final Writeup - Music Buddy**

# Synopsis

Music permeates every aspect of our lives and most everyone nowadays enjoys music through some kind of online streaming platform. Spotify in particular boasts some 345 million active monthly users on its platform alone. While the streaming service in itself does a phenomenal job of delivering the songs we love right to our headsets and speakers, it does not encapsulate the full potential music-listening-experience. By combining all that Spotify does so well with all that our ESP32 does so well, we can create a system that fuses our physical environment with the song we are listening to.

# Overview

Our goal with Music Buddy was to develop a system that could

- Receive physical input to select songs beyond just a button
- Visualize the currently playing music with lights and colors
- Facilitate some kind of social aspect between users e.g. sharing and liking songs

Some key subgoals that we wanted to tackle:

- Incorporate "rhythm of the world" by reading a heartbeat or a step count and translating it to a song with matching rhythm
- Implement seamless and dynamic notifications which would appear in near-real time and present the user with interactive options
- Visualize music in a way that combines the high level features of a song (from Spotify) with the beat-by-beat analog information from a speaker/microphone

# Video Demo

![Final Video Demo](https://youtu.be/FlR25wOP-rI)

# Functional Block Diagram

![Functional Block Diagram of our System](https://drive.google.com/uc?export=download&id=1Yo8ud_TuuhZZrhPu_yJjZEwnFxJwWAgJ)

# State Diagram

![State Diagram of our System](https://drive.google.com/uc?export=download&id=1-zkSbEXgrBXzXKZ5A27XnqF_lGQw3L1l)

# Parts List

The only additional parts we ordered were Heart Rate Sensor Modules [MAX30102](https://www.amazon.com/MAX30102-Detection-Concentration-Compatible-Arduino/dp/B07ZQNC8XP/ref=asc_df_B07ZQNC8XP/?tag=hyprod-20&linkCode=df0&hvadid=459704843517&hvpos=&hvnetw=g&hvrand=5166322811039696387&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=1018127&hvtargid=pla-914929956645&psc=1).

From the class kit, we used the accelerometer, the LCD, the ESP32, the RGB LED, three buttons, and the microphone.

# Discussion

Combine all of your milestones here to give an overview of the design process, include video demos along the way.


## BPM-based Recommendation

A central component for several of our subgoals was the ability to find songs based solely by their BPM (beats per minute). This was a little challenging at first, as it was not quite as straightforward as querying the Spotify API. In fact, Spotify offers no endpoints to search for songs by anything besides artist/title/name. So we had to explore other options. We found [this website](https://www.cs.ubc.ca/~davet/music/bpm/), which contained hyperlinks to webpages full of songs based on their BPM. We wrote a script to, given a target BPM, identify the closest actual BPM available, and then format an HTTP GET request to fetch the HTML document containing all of the songs. We then used BeautifulSoup to scrape through all of the entries and assemble a plaintext collection of songs and their artists. From there, we randomly sampled that collection to return a random song of correct BPM to the user. Putting this function on an endpoint on our server was simple enough, and from then-on we could query a endpoint on our server with just a bpm value to receive `SONG by ARTIST` in return. An example of this can be seen at the end of Michael's Week 1 Milestone video, below:

![Fetching a song from a BPM](https://youtu.be/_0jCG0o-tIE)

## Heartbeat System

The first challenge to integrating the heartbeat sensor was phsyically wiring it up with our hardware and installing the libraries so that we could actually receive data from the module. We wired it up according to the specification and downloaded and installed its associated software library from GitHub. We then created a wrapper class HeartbeatSensor, which was able to ingest raw readings from the sensor and convert it to a running average of pulse rate over time. It did this in a similar manner to how we calculated steps in lab but on a much finer scale. It kept track of the last 100 readings and calculated the mean and standard deviation for each. If any particular value was over one standard deviation of the mean, it was registered as a beat and did not come down until it fell below a half standard average again. Experimentally this worked out quite well, and when the user kept my finger still (an apparent prerequisite for this sensorâ€™s use), one could feel their pulse and see it updating on screen in realtime.

We computed the BPM by keeping track of the measurement start time and the current time and taking an average over time. From there, it was a matter of some small UI changes and sending a GET request to the existing server code which could recommend a song based on that BPM.

A particular challenge in this integration was the sensitivity of the sensor. Under perfect conditions, with the sensor truly still, the reading was pretty accurate. Unfortunately, ideal conditions were somewhat annoying to achieve. We were unable to create a final product which fully removed the need for such ideal conditions, though some options could have been to denoise the signal or not accept values above certain thresholds (heartbeats, while distinct, produced nowhere near the readings as a jolt or tap of the sensor).

An example of the functional heartbeat sensor can be seen in Michael's Week 3 and Week 4 milestone demos, below:

![Heartbeat sensor reading pulse and working in isolation](https://youtu.be/S0oJR67IOi4)

![Heartbeat sensor working with full system integration, including fetching a song by BPM](https://youtu.be/NmTJJTQTzZc)

## Music Visualization

The final visualization product combined both the spotify data and microphone input to create a pleasing visual display of a blinking and color changing LED on the ESP32.  The first step in this visualization utilized only the spotify data.  This data included information about the song as a whole like 'energy' or 'danceability', and also included information on the start time of each beat.  The first iteration of the visualization used only the start times of each beat to change the color of the LED.  The first challenge of this was to sync the LED with the current time of the song being played.  This was done through a spotify api request that returned the current progress, and then the ESP32 callibrated the LED to this progress with a small offset to account for the delay from when the request was sent until it was received.  Another challenge was that the beats data from spotify is not very dynamic, meaning that the length of each beat throughout every song was relatively the same length.  Therefore, it did not capture the fluxuations in beat length, but only captured the general beat throughout the whole song.  

To overcome the non-dynamic spotify beats data we decided to use the audio from the microphone.  This worked by having a threshold on the volume of the input, and whenever the volume exceeded this threshold, it would trigger the LED to flash with a predetermined cooldown time of around .15s during which the color wouldn't change.  One challenge with this was that the visualization acted very similarly, even on different song types since we used predetermined colors and cooldown times.

The final iteration of the visualization integrated this microphone data with the spotify data to provide a visualization taylored to each specific song.  Based on the energy and danceability of a song, the color palette and cooldown of the LED was adjusted.  For instance, low tempo songs had low energy colors and longer cooldowns compared to high tempo songs.  Ultimately, we created a dynamic LED visualization taylored to each song type.

## User Management

## Liking and sharing

The system also has networking capabilities to be able to like and share songs with friends, through privatized groups. A user may form a group, and is given a random six-digit passcode for the group. Then, any user may use the passcode to join the group, or may be invited into the group by any user already in the group. The ESP will send periodic GET requests for any invites to groups, and upon receiving a group invite, a notification will appear on the user's screen and give the user the ability to reject or accept the incoming invite.

When a user is playing a song on a connected device, the ESP can detect the song and show the song name and artist on its display. The unique Spotify Track ID is also stored, so that if the user chooses to like the song, he/she is then given a multiple choice list of the groups he/she is in, and may choose to 'like' the song in any subset of the groups, which stores the track ID in that group's liked song database. At any point, a user may choose any group he/she is in, and make a playlist out of the group's liked songs, and this playlist will automatically be added to the user's list of playlists!

Finally, users are also allowed to share songs to groups. Again, the user takes the currently playing song and can share it to any subset of groups that he/she is, and all other members of the group will receive a notification (via periodic GET requests) that he/she has been given a shared song. The user can either choose to reject the shared song, or play that song, in which case the song will be immediately played (i.e. added to the user's queue and then the track is skipped).

## Step-based Song Recommendation
The first iteration of the step-based song recommendation comprised of a feature where the user could enter the step-counter state where the IMU would start recording steps. Once the user was ready to retrieve a song that would match his or her activity level, he or she would send a request to the server (Michael's initial bpm.py script) and retrieve the recommended song.

Week 2 Demo Video: https://www.youtube.com/watch?v=_V078VdVYLY

After receiving feedback on this step-based song recommendation, we knew we had to augment our current system in a way that would make it more practical. To do so, we enhanced our recommender to choose the next song based on the number of steps in the last 30 seconds of the current song playing. To make this work, we needed the arduino to constantly send requests to spotify and receive a notification when we are in the last 30 seconds of the song, which would trigger the IMU to start recording steps. When spotify notifies we are in the last 5 seconds of the song, we then send the request to the server containing the number of steps in which to recommend a song from.

Week 3 Demo Video: https://drive.google.com/drive/u/0/folders/11i0vQEWJTgJ1S8OaQ5a-rSdf-kirrFpg

I then integrated this feature into the central state machine, where the user has the option from the home menu to enter the step-recommender.

Week 4 Demo Video: https://drive.google.com/drive/u/0/folders/1H6U7JlpMs-waiMY2MWFtXkzIjnB9MXOE



# Code Overview

## Python Code

### `bpm.py`

This is the script which contained all of our web scraping as defined in [the BPM section](#bpm-basedrecommendation). It accepts an HTTP GET request, sends a request to the aforementioned song website, scrapes it to extract songs, chooses a random one, and returns it.



### `get_spotify_data.py`

This is the script which contains the functions for the user to get all spotify data for a specific user. Specifically, this script handles the periodic GET requests from the user's status requests. The user is notified when there is exactly 32 seconds left in the song, which triggers its IMU to start recording steps. It then notifies the user when the current song playing on spotify is in the last 2 second of the song, where the esp32 would be notified to send the server the current number of steps. At this point, the GET request would contain the number of steps. The script parses this request and sends the number of steps to the 'recommend_song_by_bpm' function with an offset of 80. The recommended song is then returned.

### PUT PYTHON FILES HERE WITH ~ PARAGRAPH EXPLANATION OF FUNCTION

## Arduino Code

### `project_arduino.ino`

This file housed nearly all of our arduino code. In our `setup()` we initialize all of our sensors, timers, and output IO pins.

In `loop()` we take readings from our buttons, our microphone, and our heartbeat sensor. We then take all of this information and feed it to `handleDisplay()` which handled state and display for our system.

#### `handleDisplay()`

This is the function that housed our state machine. States were further compartmentalized into their own functions, which we will go over in the following sections. The entire function was quite compact due to the way that we wrote our code.

```cpp
void handleDisplay(int leftReading, int middleReading, int rightReading) {
    fetchNotifications();
    
    tft.setCursor(0,0,1); // reset cursor at the top of the screen
  
    if (state_change) {
      tft.fillScreen(TFT_BLACK);
      state_change = false;
    }
  
    switch(state){
  
      case idle: //Menu State
          idleState(leftReading, middleReading, rightReading);
          break;
      case song_menu: //Here the user has option to fetch vis or display song name
          songMenuState(leftReading, middleReading, rightReading);
          break;
      case vis_menu: //Vis menu
          visMenuState(leftReading, middleReading, rightReading);
          break;  
      case groups_menu: // Groups menu
          groupsMenuState(leftReading, middleReading, rightReading);
          break;
      case pulse_recommendation:
          pulseRecommendationState(leftReading, middleReading, rightReading);
          break;
      case recommended_song:
          recommendedSongState(leftReading, middleReading, rightReading);
          break;
    }
  }
```


#### `idleState()`

This was simply the state that greeted users on system start. They were met with instructions to press the left button to proceed to the Song Menu. Pressing the left button transitioned to the Song Menu state. On transition, our system fetched the current playing song.

#### `songMenuState()`

This was the "hub" of our system. If there was a song playing, it was displayed at the top of the screen. If there was a BPM detected, it was displayed beneath the song.

This is also where notifications were displayed, with the logic governing them based on whether or not the notification strings were empty or had content.

Based on user input via the buttons, we sent them off to other states, with instructions describing how to transition and come back.

#### `visMenuState()`

When this state is entered, a request is made to the server to get the features of the current song being played that determine the tempo and colors of the LED.  Then, the LED displays its visualization as explained above.  

One challenge was that at first, we tried to have the LED visualization constantly running even while in the other states.  However this did not work because of the delays in the code from the other functionalities.  In order to make the LED visualizaition as precise as possible, we had to create its own state with a while loop that ran the visualization until the user performed a short left press to exit and go back to the songMenuState.

#### `groupsMenuState()`

#### `pulseRecommendationState()`

This was a one-cycle state. Upon entry, an HTTP request targetting our BPM endpoint was formed and sent via `getSongByBPM()`. Upon receiving a response, the user was sent to the recommendedSongState.

#### `recommendedSongState()`

In this state, the user was presented with the recommended song on the display. They could return to the home screen by pressing a button.

#### OTHER FUNCTIONS YOU GUYS WANT TO TALK ABOUT

### `Button.cpp`

This was the out-of-the-box button code that we have used all semester.

### `HeartbeatSensor.cpp`

This file contained a class which wrapped our Heartbeat Sensor module, ingesting a reading every loop and computing a running average BPM from it. It used a sliding window to keep track of beats and the relative time deltas with which it computed BPM as `bpm = NUM_PULSES / TIME_ELAPSED`.

## Database Overview

what were the entries in our databse? what did they look like? example?

# Thank you!

A big thank you from the whole team for an awesome class! - Raul, Michael, Dan, James, Mark





<style class="fallback">body{visibility:hidden}</style><script>markdeepOptions={tocStyle:'medium'};</script>
<!-- Markdeep: --><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>
